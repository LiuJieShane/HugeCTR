{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.48.2)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.19.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/mnt/dldata/vinhn/DeepLearningExamples/PyTorch/Recommendation/NCF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import pandas as pd\n",
    "from load import implicit_load\n",
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_RATINGS = 20\n",
    "USER_COLUMN = 'user_id'\n",
    "ITEM_COLUMN = 'item_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _TestNegSampler:\n",
    "    def __init__(self, train_ratings, nb_neg):\n",
    "        self.nb_neg = nb_neg\n",
    "        self.nb_users = int(train_ratings[:, 0].max()) + 1\n",
    "        self.nb_items = int(train_ratings[:, 1].max()) + 1\n",
    "\n",
    "        # compute unique ids for quickly created hash set and fast lookup\n",
    "        ids = (train_ratings[:, 0] * self.nb_items) + train_ratings[:, 1]\n",
    "        self.set = set(ids)\n",
    "\n",
    "    def generate(self, batch_size=128*1024):\n",
    "        users = torch.arange(0, self.nb_users).reshape([1, -1]).repeat([self.nb_neg, 1]).transpose(0, 1).reshape(-1)\n",
    "\n",
    "        items = [-1] * len(users)\n",
    "\n",
    "        random_items = torch.LongTensor(batch_size).random_(0, self.nb_items).tolist()\n",
    "        print('Generating validation negatives...')\n",
    "        for idx, u in enumerate(tqdm.tqdm(users.tolist())):\n",
    "            if not random_items:\n",
    "                random_items = torch.LongTensor(batch_size).random_(0, self.nb_items).tolist()\n",
    "            j = random_items.pop()\n",
    "            while u * self.nb_items + j in self.set:\n",
    "                if not random_items:\n",
    "                    random_items = torch.LongTensor(batch_size).random_(0, self.nb_items).tolist()\n",
    "                j = random_items.pop()\n",
    "\n",
    "            items[idx] = j\n",
    "        items = torch.LongTensor(items)\n",
    "        return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000263 ratings on 26744 items from 138493 users from 1995-01-09 11:46:44 to 2015-03-31 06:40:02\n",
      "Filtering out users with less than 20 ratings\n",
      "Mapping original user and item IDs to new sequential IDs\n"
     ]
    }
   ],
   "source": [
    "df = implicit_load('/mnt/dldata/vinhn/DeepLearningExamples/PyTorch/Recommendation/NCF/data/ml-20m/ratings.csv', sort=False)\n",
    "\n",
    "print(\"Filtering out users with less than {} ratings\".format(MIN_RATINGS))\n",
    "grouped = df.groupby(USER_COLUMN)\n",
    "df = grouped.filter(lambda x: len(x) >= MIN_RATINGS)\n",
    "\n",
    "print(\"Mapping original user and item IDs to new sequential IDs\")\n",
    "df[USER_COLUMN], unique_users = pd.factorize(df[USER_COLUMN])\n",
    "df[ITEM_COLUMN], unique_items = pd.factorize(df[ITEM_COLUMN])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              0\n",
       "1              1\n",
       "2              2\n",
       "3              3\n",
       "4              4\n",
       "            ... \n",
       "20000258    1814\n",
       "20000259    1037\n",
       "20000260    3950\n",
       "20000261    1818\n",
       "20000262    4010\n",
       "Name: item_id, Length: 20000263, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[ITEM_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([     1,      2,      3,      4,      5,      6,      7,      8,\n",
       "                 9,     10,\n",
       "            ...\n",
       "            138484, 138485, 138486, 138487, 138488, 138489, 138490, 138491,\n",
       "            138492, 138493],\n",
       "           dtype='int64', length=138493)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([     2,     29,     32,     47,     50,    112,    151,    223,\n",
       "               253,    260,\n",
       "            ...\n",
       "            104307, 106170, 106401, 113539, 118856, 121017, 121019, 121021,\n",
       "            110167, 110510],\n",
       "           dtype='int64', length=26744)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./mappings.pickle', 'wb') as handle:\n",
    "    pickle.dump({\"users\": unique_users, \"items\": unique_items}, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to sort before popping to get last item\n",
    "df.sort_values(by='timestamp', inplace=True)\n",
    "\n",
    "# clean up data\n",
    "del df['rating'], df['timestamp']\n",
    "df = df.drop_duplicates() # assuming it keeps order\n",
    "\n",
    "# now we have filtered and sorted by time data, we can split test data out\n",
    "grouped_sorted = df.groupby(USER_COLUMN, group_keys=False)\n",
    "test_data = grouped_sorted.tail(1).sort_values(by='user_id')\n",
    "# need to pop for each group\n",
    "train_data = grouped_sorted.apply(lambda x: x.iloc[:-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: no way to keep reference training data ordering because use of python set and multi-process\n",
    "# It should not matter since it will be later randomized again\n",
    "# save train and val data that is fixed.\n",
    "train_ratings = torch.from_numpy(train_data.values)\n",
    "torch.save(train_ratings, './train_ratings.pt')\n",
    "test_ratings = torch.from_numpy(test_data.values)\n",
    "torch.save(test_ratings, './test_ratings.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19861770, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0,     62],\n",
       "        [     1,     15],\n",
       "        [     2,    336],\n",
       "        ...,\n",
       "        [138490,    173],\n",
       "        [138491,    204],\n",
       "        [138492,    695]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 147596/13849300 [00:00<00:09, 1475954.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating validation negatives...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13849300/13849300 [00:08<00:00, 1688484.61it/s]\n"
     ]
    }
   ],
   "source": [
    "sampler = _TestNegSampler(train_ratings.cpu().numpy(), 100)  # using 100 negative samples\n",
    "test_negs = sampler.generate().cuda()\n",
    "test_negs = test_negs.reshape(-1, 100)\n",
    "torch.save(test_negs, './test_negatives.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([138493, 100])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_negs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13526,  6010,  2580, 20795, 17019, 13412,  9002,   278,  2323, 15147,\n",
       "         6525,  9238,  6427,  5638, 21305,  8535, 22253,   680,  6121, 23959,\n",
       "        12321,  4001,  1903,  4116,  4869, 20119, 12954,  2814, 20602, 12807,\n",
       "        15583,  1976, 26648,  6573,  8468, 21415, 24641, 13358,  9048, 23819,\n",
       "        17701,   986, 11426, 11518,  8002, 18193,  5919,  2840, 20801, 22531,\n",
       "        22351, 18287, 22100,  5961, 24353, 18816, 13567,   585,  7582, 10957,\n",
       "        12966, 12392, 16984, 23337, 23803, 10115,  9488, 23602, 26272, 12293,\n",
       "        19981, 16574, 16295,  7151, 10912,  3921,  8372, 16605, 21906,  7044,\n",
       "         8300, 14418,  2519,  8738,  8490, 24165, 20467, 12010,  9091, 20323,\n",
       "        13288, 18053,  5985, 17934, 11163, 15424,  5963, 13823,  1443, 25891],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_negs[1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
